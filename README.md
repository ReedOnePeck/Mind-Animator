# <p align="center"> Mind-Animator </p> 
This is the official code for the paper "Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity"[**ICLR 2025**] [Project page](https://dl.acm.org/doi/10.1145/3581783.3613832)

## <p align="center">  Related works  </p> 
![](https://github.com/ReedOnePeck/Mind-Animator/blob/main/images/related_works.png)<br>

## <p align="center">  Schematic diagram of Mind-Animator  </p> 
![](https://github.com/ReedOnePeck/Mind-Animator/blob/main/images/method.png)<br>


## <p align="center">  News  </p> 
- Jan. 25, 2025. Project release.
- Jan. 23, 2025. Our paper is accpeted at ICLR2025!

# <p align="center"> Steps to reproduce Mind-Animator </p> 
## <p align="center">  Preliminaries  </p> 
This code was developed and tested with:

*  Python version 3.9.16
*  PyTorch version 1.12.1
*  A100 80G

## <p align="center">  Environment setup  </p> 
Create and activate conda environment named ```Mind-animator``` from our ```environment_MA.yml```
```sh
conda env create -f environment_MA.yml
conda activate Mind-animator
```

Since our project is built on Tune-a-video, if you encounter issues with the above commands, you can also follow the steps below one by one.

*  Create a virtual environment for Tune-a-video.
```sh
pip install -r Tune-a-video-requirements.txt
```
*  Install CLIP.
```sh
pip install ftfy regex tqdm
pip install git+https://github.com/openai/CLIP.git
```
*  Install the remaining packages as needed.

## <p align="center">  Data preparation  </p> 
*  Dataset download.

The open-source datasets used in this paper can be accessed via the following links:

(1) CC2017: https://purr.purdue.edu/publications/2809/1

(2) HCP: https://www.humanconnectome.org/

(3) Algonauts2021: http://algonauts.csail.mit.edu/2021/index.html

```
/data
â”£ ðŸ“‚ CC2017_Purdue
â”ƒ   â”£ ðŸ“‚ Stimuli/video_fmri_dataset/stimuli/
â”ƒ   â”ƒ   â”£ ðŸ“œ seg1.mp4
â”ƒ   â”ƒ   â”£ ðŸ“œ seg2.mp4
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”ƒ   â”£ ðŸ“œ seg18.mp4
â”ƒ   â”ƒ   â”£ ðŸ“œ test1.mp4
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”ƒ   â”£ ðŸ“œ test5.mp4
â”ƒ   â”£ ðŸ“‚ Subject01/video_fmri_dataset/subject1
â”ƒ   â”ƒ   â”£ ðŸ“‚ fmri
â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“‚ seg1
â”ƒ   â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“‚ cifti
â”ƒ   â”ƒ   â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“œ seg1_1_Atlas.dtseries.nii
â”ƒ   â”ƒ   â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“œ seg1_2_Atlas.dtseries.nii
â”ƒ   â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“‚ mni
â”ƒ   â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“‚ raw
â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“‚ seg2
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“‚ test5
â”ƒ   â”ƒ   â”£ ðŸ“‚ smri
â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“œ t1w.nii.gz
â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“œ t2w.nii.gz


â”£ ðŸ“‚ HCP
â”ƒ   â”£ ðŸ“‚ Stumuli_videos
â”ƒ   â”ƒ   â”— ðŸ“œ clip_1.mp4
â”ƒ   â”ƒ   â”— ðŸ“œ clip_1.mp4
â”ƒ   â”ƒ   â”— ...
â”ƒ   â”ƒ   â”— ðŸ“œ clip_3040.mp4
â”ƒ   â”£ ðŸ“‚ fMRI_response_surface
â”ƒ   â”ƒ   â”£ ðŸ“‚ 100610
â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“œ preprocessed_fMRI.npy
â”ƒ   â”ƒ   â”£ ðŸ“‚ 102816
â”ƒ   â”ƒ   â”£ ðŸ“‚ 104416


â”£ ðŸ“‚ Algonauts2021_data
â”ƒ   â”£ ðŸ“‚ AlgonautsVideos268_All_30fpsmax
â”ƒ   â”ƒ   â”£ ðŸ“œ 0001_0-0-1-6-7-2-8-0-17500167280.mp4
â”ƒ   â”ƒ   â”£ ðŸ“œ 0002_0-0-4-3146384004.mp4
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”ƒ   â”£ ðŸ“œ 1102_meta_R-5602303_250.mp4
â”ƒ   â”£ ðŸ“‚ participants_data_v2021
â”ƒ   â”ƒ   â”£ ðŸ“‚ sub01
â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“œ EBA.pkl
â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“œ FFA.pkl
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”ƒ   â”ƒ   â”£ ðŸ“œ V4.pkl
â”ƒ   â”ƒ   â”£ ðŸ“‚ sub02
â”ƒ   â”ƒ   â”ƒ   â”— ...
â”ƒ   â”ƒ   â”£ ðŸ“‚ sub10
```


